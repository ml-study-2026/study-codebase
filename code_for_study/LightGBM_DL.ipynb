{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e71ce-6821-4a42-be90-1b01b5eacd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = r\"C:\\Users\\***\"\n",
    "\n",
    "print(\"Files in DATA_DIR:\")\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if f.lower().endswith((\".xlsx\", \".xls\", \".csv\")):\n",
    "        print(\"  -\", f)\n",
    "\n",
    "DATA_FILE = \"merged_all.xlsx\"\n",
    "\n",
    "path = os.path.join(DATA_DIR, DATA_FILE)\n",
    "\n",
    "if DATA_FILE.lower().endswith((\".xlsx\", \".xls\")):\n",
    "    df = pd.read_excel(path)\n",
    "else:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "print(\"Loaded df.shape =\", df.shape)\n",
    "print(df.head(2))\n",
    "print(\"\\nColumns sample:\", list(df.columns)[:15])\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, PredefinedSplit\n",
    "\n",
    "RANDOM_STATE = 2025\n",
    "TARGET_COL = \"Level_OA\"\n",
    "\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise ValueError(\n",
    "        f\"Target column '{TARGET_COL}' not found. \"\n",
    "        f\"Available columns (first 30): {df.columns.tolist()[:30]} ...\"\n",
    "    )\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "df[\"fold_id\"] = -1\n",
    "for fold, (_, val_idx) in enumerate(skf.split(df, df[TARGET_COL])):\n",
    "    df.loc[val_idx, \"fold_id\"] = fold\n",
    "\n",
    "df[\"fold_id\"] = df[\"fold_id\"].astype(int)\n",
    "print(\"\\nfold_id counts:\\n\", df[\"fold_id\"].value_counts().sort_index())\n",
    "\n",
    "cv = PredefinedSplit(test_fold=df[\"fold_id\"].values)\n",
    "\n",
    "# ==============================\n",
    "DROP_COLS = [\"Name\", TARGET_COL, \"fold_id\", \"PP_OA\"]\n",
    "DROP_COLS = [c for c in DROP_COLS if c in df.columns]\n",
    "feature_cols_xyz = [c for c in df.columns if c not in DROP_COLS]\n",
    "\n",
    "Y_COLS = [\"G\"]\n",
    "Z_COLS = [\"P_PPAR\", \"P_PI3K\", \"P_PPAR\", \"P_ROS\", \"P_LPS\"]\n",
    "\n",
    "for c in (Y_COLS + Z_COLS):\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(\n",
    "            f\"Column '{c}' is not in df. Please update Y_COLS/Z_COLS \"\n",
    "            f\"to match the real column names in your dataset.\"\n",
    "        )\n",
    "\n",
    "feature_cols_x = [c for c in feature_cols_xyz if c not in set(Y_COLS + Z_COLS)]\n",
    "feature_cols_xy = [c for c in feature_cols_xyz if c not in set(Z_COLS)]\n",
    "feature_cols_xz = [c for c in feature_cols_xyz if c not in set(Y_COLS)]\n",
    "\n",
    "print(\"\\n#features:\")\n",
    "print(\"  X-only:\", len(feature_cols_x))\n",
    "print(\"  X+Y:\", len(feature_cols_xy))\n",
    "print(\"  X+Z:\", len(feature_cols_xz))\n",
    "print(\"  X+Y+Z:\", len(feature_cols_xyz))\n",
    "\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "lgbm = LGBMClassifier(\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=300,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", lgbm),\n",
    "])\n",
    "\n",
    "scoring = {\n",
    "    \"acc\": \"accuracy\",\n",
    "    \"f1\": \"f1_macro\",\n",
    "    \"auc\": \"roc_auc_ovr\",\n",
    "}\n",
    "\n",
    "def eval_feature_set(cols, name):\n",
    "    X = df[cols].values\n",
    "    y = df[TARGET_COL].values\n",
    "    out = cross_validate(\n",
    "        pipe, X, y,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "    return {\n",
    "        \"FeatureSet\": name,\n",
    "        \"n_features\": len(cols),\n",
    "        \"Accuracy_mean\": out[\"test_acc\"].mean(),\n",
    "        \"Accuracy_std\": out[\"test_acc\"].std(),\n",
    "        \"MacroF1_mean\": out[\"test_f1\"].mean(),\n",
    "        \"MacroF1_std\": out[\"test_f1\"].std(),\n",
    "        \"MacroAUC_mean\": out[\"test_auc\"].mean(),\n",
    "        \"MacroAUC_std\": out[\"test_auc\"].std(),\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "rows.append(eval_feature_set(feature_cols_x, \"X-only\"))\n",
    "rows.append(eval_feature_set(feature_cols_xy, \"X+Y\"))\n",
    "rows.append(eval_feature_set(feature_cols_xz, \"X+Z\"))\n",
    "rows.append(eval_feature_set(feature_cols_xyz, \"X+Y+Z\"))\n",
    "\n",
    "res_df = pd.DataFrame(rows)\n",
    "\n",
    "def pm(m, s, nd=3):\n",
    "    return f\"{m:.{nd}f}±{s:.{nd}f}\"\n",
    "\n",
    "pretty = pd.DataFrame({\n",
    "    \"FeatureSet\": res_df[\"FeatureSet\"],\n",
    "    \"n_features\": res_df[\"n_features\"],\n",
    "    \"Accuracy\": [pm(m, s) for m, s in zip(res_df[\"Accuracy_mean\"], res_df[\"Accuracy_std\"])],\n",
    "    \"Macro-F1\": [pm(m, s) for m, s in zip(res_df[\"MacroF1_mean\"], res_df[\"MacroF1_std\"])],\n",
    "    \"Macro-AUC\": [pm(m, s) for m, s in zip(res_df[\"MacroAUC_mean\"], res_df[\"MacroAUC_std\"])],\n",
    "})\n",
    "\n",
    "print(\"\\n=== LightGBM 10-fold (fixed folds) ===\")\n",
    "print(pretty)\n",
    "\n",
    "out_path = os.path.join(DATA_DIR, \"lgbm_ablation_results.xlsx\")\n",
    "pretty.to_excel(out_path, index=False)\n",
    "print(\"\\nSaved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff7dcf-30d8-46bf-bba9-b5e5ce3007b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "SEED = 2025\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "PATH_MERGED = r\"C:\\PY3\\pythonProject\\merged_all.xlsx\"\n",
    "\n",
    "PATH_IN_7565 = r\"C:\\PY3\\pythonProject\\7565.xlsx\"\n",
    "PATH_OUT_7565 = rf\"C:\\PY3\\pythonProject\\7565_filled_{time.strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "\n",
    "LABEL_COL = \"Level_OA\"\n",
    "NAME_COL = \"Name\"\n",
    "FOLD_COL = \"fold_id\"\n",
    "N_SPLITS = 10\n",
    "\n",
    "def nan_inf_to_nan(X: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    X = X.astype(float, copy=True)\n",
    "    X[~np.isfinite(X)] = np.nan\n",
    "    return X\n",
    "\n",
    "\n",
    "def mean_impute_fit_transform(X_train_raw: np.ndarray):\n",
    "\n",
    "    Xn = nan_inf_to_nan(X_train_raw)\n",
    "    col_mean = np.nanmean(Xn, axis=0)\n",
    "\n",
    "    col_mean = np.where(np.isfinite(col_mean), col_mean, 0.0)\n",
    "\n",
    "    inds = np.where(np.isnan(Xn))\n",
    "    Xn[inds] = np.take(col_mean, inds[1])\n",
    "    return Xn.astype(np.float32), col_mean.astype(np.float32)\n",
    "\n",
    "\n",
    "def mean_impute_transform(X_raw: np.ndarray, col_mean: np.ndarray):\n",
    "    Xn = nan_inf_to_nan(X_raw)\n",
    "    inds = np.where(np.isnan(Xn))\n",
    "    Xn[inds] = np.take(col_mean, inds[1])\n",
    "    return Xn.astype(np.float32)\n",
    "df = pd.read_excel(PATH_MERGED)\n",
    "assert LABEL_COL in df.columns, f\"缺列：{LABEL_COL}\"\n",
    "df[LABEL_COL] = df[LABEL_COL].astype(int)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "df[FOLD_COL] = -1\n",
    "for fold, (_, val_idx) in enumerate(skf.split(df, df[LABEL_COL])):\n",
    "    df.loc[val_idx, FOLD_COL] = fold\n",
    "\n",
    "print(\"Number of rows:\", len(df))\n",
    "print(\"Fold distribution:\", df[FOLD_COL].value_counts().to_dict())\n",
    "\n",
    "\n",
    "baseline_drop = {NAME_COL, LABEL_COL, FOLD_COL, \"P_OA\"}  # 跟你之前一致\n",
    "baseline_features = [c for c in df.columns if c not in baseline_drop]\n",
    "\n",
    "X_base = df[baseline_features]\n",
    "y_base = df[LABEL_COL].values\n",
    "\n",
    "lgbm = LGBMClassifier(\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=300,\n",
    "    random_state=SEED,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), \n",
    "    (\"model\", lgbm)\n",
    "])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "cv_results = cross_validate(\n",
    "    pipe, X_base, y_base, cv=cv,\n",
    "    scoring=[\"accuracy\", \"f1_macro\", \"roc_auc_ovr\"],\n",
    "    return_train_score=False\n",
    ")\n",
    "print(\"Accuracy:\", cv_results[\"test_accuracy\"].mean())\n",
    "print(\"Macro-F1:\", cv_results[\"test_f1_macro\"].mean())\n",
    "print(\"AUC:\", cv_results[\"test_roc_auc_ovr\"].mean())\n",
    "\n",
    "need_cols = [\"G\", \"P_PI3K\", \"P_PPAR\", \"P_ROS\", \"P_LPS\"]\n",
    "for c in need_cols:\n",
    "    assert c in df.columns, f\"缺列：{c}\"\n",
    "\n",
    "exclude_for_X = {NAME_COL, LABEL_COL, FOLD_COL, \"G\", \"P_PI3K\", \"P_PPAR\", \"P_ROS\", \"P_LPS\", \"P_OA\"}\n",
    "X_cols = [c for c in df.columns if c not in exclude_for_X and np.issubdtype(df[c].dtype, np.number)]\n",
    "print(f\"结构描述符个数: {len(X_cols)}\")\n",
    "\n",
    "\n",
    "def feature_columns(mode: str):\n",
    "    if mode == \"X-only\":\n",
    "        return X_cols\n",
    "    if mode == \"X+Y\":\n",
    "        return X_cols + [\"G\"]\n",
    "    if mode == \"X+Z\":\n",
    "        return X_cols + [\"P_PI3K\", \"P_PPAR\", \"P_ROS\", \"P_LPS\"]\n",
    "    if mode == \"X+Y+Z\":\n",
    "        return X_cols + [\"G\", \"P_PI3K\", \"P_PPAR\", \"P_ROS\", \"P_LPS\"]\n",
    "    raise ValueError(mode)\n",
    "\n",
    "\n",
    "class TinyTabTransformer(nn.Module):\n",
    "    def __init__(self, in_dim, n_classes=3, d_model=64, n_heads=4, n_layers=2, dropout=0.35):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads,\n",
    "            dim_feedforward=d_model * 2,\n",
    "            dropout=dropout, batch_first=True, activation=\"gelu\"\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).unsqueeze(1)\n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x.squeeze(1))\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "def train_one_fold(train_X, train_y, val_X, val_y,\n",
    "                   epochs=500, batch_size=16, lr=1e-3, weight_decay=1e-4,\n",
    "                   d_model=64, n_heads=4, n_layers=2, dropout=0.35, patience=50):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n_classes = len(np.unique(train_y))\n",
    "\n",
    "    model = TinyTabTransformer(\n",
    "        in_dim=train_X.shape[1], n_classes=n_classes,\n",
    "        d_model=d_model, n_heads=n_heads, n_layers=n_layers, dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    classes, counts = np.unique(train_y, return_counts=True)\n",
    "    weight_map = {c: (np.sum(counts) / cnt) for c, cnt in zip(classes, counts)}\n",
    "    weights = torch.tensor([weight_map[i + 1] for i in range(n_classes)],\n",
    "                           dtype=torch.float32).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_ds = TensorDataset(torch.tensor(train_X, dtype=torch.float32),\n",
    "                             torch.tensor(train_y - 1, dtype=torch.long))\n",
    "    val_ds = TensorDataset(torch.tensor(val_X, dtype=torch.float32),\n",
    "                           torch.tensor(val_y - 1, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    best_f1, best_state, no_improve = -1.0, None, 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for bx, by in train_loader:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            logits = model(bx)\n",
    "            loss = criterion(logits, by)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        all_prob, all_pred, all_true = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for bx, by in val_loader:\n",
    "                bx = bx.to(device)\n",
    "                logits = model(bx)\n",
    "                prob = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "                pred = prob.argmax(axis=1)\n",
    "                all_prob.append(prob)\n",
    "                all_pred.append(pred)\n",
    "                all_true.append(by.numpy())\n",
    "\n",
    "        y_true = np.concatenate(all_true)\n",
    "        y_pred = np.concatenate(all_pred)\n",
    "        y_prob = np.concatenate(all_prob)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model(torch.tensor(val_X, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "        val_prob = softmax(val_logits, axis=1)\n",
    "        val_pred = val_prob.argmax(axis=1)\n",
    "\n",
    "    y_true0 = (val_y - 1)\n",
    "    acc = accuracy_score(y_true0, val_pred)\n",
    "    f1 = f1_score(y_true0, val_pred, average=\"macro\")\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(label_binarize(y_true0, classes=[0, 1, 2]),\n",
    "                            val_prob, average=\"macro\", multi_class=\"ovr\")\n",
    "    except:\n",
    "        auc = np.nan\n",
    "\n",
    "    return acc, f1, auc\n",
    "\n",
    "\n",
    "modes = [\"X-only\", \"X+Y\", \"X+Z\", \"X+Y+Z\"]\n",
    "results = {}\n",
    "\n",
    "for mode in modes:\n",
    "    cols = feature_columns(mode)\n",
    "    print(f\"\\n=== Mode: {mode} | Number of features: {len(cols)} ===\")\n",
    "\n",
    "    acc_list, f1_list, auc_list = [], [], []\n",
    "\n",
    "    for fold in range(N_SPLITS):\n",
    "        train_idx = df[FOLD_COL] != fold\n",
    "        val_idx = df[FOLD_COL] == fold\n",
    "\n",
    "        X_train_raw = df.loc[train_idx, cols].values\n",
    "        y_train = df.loc[train_idx, LABEL_COL].values\n",
    "        X_val_raw = df.loc[val_idx, cols].values\n",
    "        y_val = df.loc[val_idx, LABEL_COL].values\n",
    "\n",
    "        X_train_imp, col_mean = mean_impute_fit_transform(X_train_raw)\n",
    "        X_val_imp = mean_impute_transform(X_val_raw, col_mean)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train_imp).astype(np.float32)\n",
    "        X_val = scaler.transform(X_val_imp).astype(np.float32)\n",
    "\n",
    "        acc, f1, auc = train_one_fold(X_train, y_train, X_val, y_val)\n",
    "        acc_list.append(acc)\n",
    "        f1_list.append(f1)\n",
    "        auc_list.append(auc)\n",
    "\n",
    "    results[mode] = {\n",
    "        \"acc_mean\": np.mean(acc_list),\n",
    "        \"acc_std\": np.std(acc_list),\n",
    "        \"f1_mean\": np.mean(f1_list),\n",
    "        \"f1_std\": np.std(f1_list),\n",
    "        \"auc_mean\": np.nanmean(auc_list),\n",
    "        \"auc_std\": np.nanstd(auc_list),\n",
    "    }\n",
    "\n",
    "    print(f\"[{mode}] 10-fold 结果：\"\n",
    "          f\"Acc={np.mean(acc_list):.3f}±{np.std(acc_list):.3f} | \"\n",
    "          f\"F1={np.mean(f1_list):.3f}±{np.std(f1_list):.3f} | \"\n",
    "          f\"AUC={np.nanmean(auc_list):.3f}±{np.nanstd(auc_list):.3f}\")\n",
    "\n",
    "print(pd.DataFrame(results).T)\n",
    "\n",
    "if not os.path.exists(PATH_IN_7565):\n",
    "print(f\"[Skipped] File not found: {PATH_IN_7565}\")\n",
    "else:\n",
    "    df_big = pd.read_excel(PATH_IN_7565)\n",
    "\n",
    "    num_cols = [c for c in df_big.columns if np.issubdtype(df_big[c].dtype, np.number)]\n",
    "    # 只把 inf/-inf 当缺失，不把0当缺失\n",
    "    df_big[num_cols] = df_big[num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 用整张表（大样本自身）的列均值填满（如果你想用训练集均值去填大样本，需要另写）\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    df_big[num_cols] = imputer.fit_transform(df_big[num_cols])\n",
    "\n",
    "    os.makedirs(os.path.dirname(PATH_OUT_7565), exist_ok=True)\n",
    "    df_big.to_excel(PATH_OUT_7565, index=False)\n",
    "    print(f\"[OK] 已输出: {PATH_OUT_7565}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50769a07-9a4b-4fd2-8c56-28e1249c7611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
