{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a577533-901e-414f-a84e-f04a5a2dc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.special import softmax\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "PATH = r\"C:\\**\\**\\\"\n",
    "try:\n",
    "    df = pd.read_excel(PATH, engine=\"openpyxl\")\n",
    "except:\n",
    "    import xlrd\n",
    "    df = pd.read_excel(PATH, engine=\"xlrd\")\n",
    "\n",
    "exclude = {\"Name\",\"Level_OA\",\"fold_id\",\"G\",\"P_PI3K\",\"P_PPAR\",\"P_ROS\",\"P_LPS\",\"P_OA\",\"PF_C_num\"}\n",
    "X_cols = [c for c in df.columns if c not in exclude and df[c].dtype != 'O']\n",
    "X_df = df[X_cols].copy()\n",
    "y = df[\"Level_OA\"].astype(int).values\n",
    "print(f: {len(X_cols)}\")\n",
    "print(\"\", X_cols[:8])\n",
    "\n",
    "class TinyTabTransformer(nn.Module):\n",
    "    def __init__(self, in_dim, n_classes=3, d_model=64, n_heads=4, n_layers=2, dropout=0.35):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_model*2,\n",
    "            dropout=dropout, batch_first=True, activation=\"gelu\"\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model//2, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).unsqueeze(1)\n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x.squeeze(1))\n",
    "        return self.head(x)\n",
    "\n",
    "def fit_tiny_model(X, y, seed=2025, val_size=0.1, epochs=500, patience=50,\n",
    "                   d_model=64, n_heads=4, n_layers=2, dropout=0.35,\n",
    "                   lr=1e-3, weight_decay=1e-4, batch_size=16):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=seed)\n",
    "    train_idx, val_idx = next(sss.split(X, y))\n",
    "    X_tr, X_val = X[train_idx], X[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_tr = scaler.fit_transform(X_tr)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TinyTabTransformer(in_dim=X_tr.shape[1], n_classes=3,\n",
    "                               d_model=d_model, n_heads=n_heads,\n",
    "                               n_layers=n_layers, dropout=dropout).to(device)\n",
    "\n",
    "    classes, counts = np.unique(y_tr, return_counts=True)\n",
    "    weight_map = {c: (np.sum(counts)/cnt) for c,cnt in zip(classes, counts)}\n",
    "    weights = torch.tensor([weight_map[i+1] for i in range(3)], dtype=torch.float32).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    tr_ds = TensorDataset(torch.tensor(X_tr, dtype=torch.float32),\n",
    "                          torch.tensor(y_tr-1, dtype=torch.long))\n",
    "    va_ds = TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                          torch.tensor(y_val-1, dtype=torch.long))\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "    va_loader = DataLoader(va_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    best_f1, best_state, no_imp = -1.0, None, 0\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for bx, by in tr_loader:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            loss = criterion(model(bx), by)\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        all_pred, all_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for bx, by in va_loader:\n",
    "                logits = model(bx.to(device)).cpu().numpy()\n",
    "                pred = logits.argmax(axis=1)\n",
    "                all_pred.append(pred); all_true.append(by.numpy())\n",
    "        y_true = np.concatenate(all_true)\n",
    "        y_pred = np.concatenate(all_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1; best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}; no_imp = 0\n",
    "        else:\n",
    "            no_imp += 1\n",
    "            if no_imp >= patience: break\n",
    "\n",
    "    model.load_state_dict({k:v for k,v in best_state.items()})\n",
    "    return model.to(device), scaler, (train_idx, val_idx), best_f1\n",
    "\n",
    "X_all = X_df.values\n",
    "model, scaler, (tr_idx, va_idx), best_f1 = fit_tiny_model(X_all, y)\n",
    "print(f\" Macro-F1≈{best_f1:.3f}\")\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "def predict_proba_level3(X_in_np):\n",
    "    X_std = scaler.transform(X_in_np)\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.tensor(X_std, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "    prob = softmax(logits, axis=1)\n",
    "    return prob[:, 2]  \n",
    "\n",
    "bg = shap.kmeans(X_df.values, 30)\n",
    "explainer = shap.KernelExplainer(predict_proba_level3, bg)\n",
    "X_explain = X_df.values\n",
    "shap_values = explainer.shap_values(X_explain)\n",
    "X_explain_df = pd.DataFrame(X_explain, columns=X_cols)\n",
    "\n",
    "outdir = r\"C:\\**\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "shap.summary_plot(shap_values, X_explain_df, plot_type=\"bar\", show=False, max_display=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(outdir, \"global_feature_importance_bar_descriptors_top20.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "shap.summary_plot(shap_values, X_explain_df, show=False, max_display=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(outdir, \"shap_beeswarm_descriptors_top20.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "mean_abs = np.abs(shap_values).mean(axis=0)\n",
    "order = np.argsort(-mean_abs)\n",
    "topK = 6\n",
    "top_feats = [X_cols[i] for i in order[:topK]]\n",
    "for feat in top_feats:\n",
    "    plt.figure(figsize=(5.2,4.2))\n",
    "    shap.dependence_plot(feat, shap_values, X_explain_df, show=False, interaction_index=None)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, f\"dep_{feat}.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7d574-f6fd-437e-9c30-c85e90a587e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "outdir = r\"C:\\Users\\81005\\Desktop\\CYH\\3-PFAS\\深度学习验证\\fig_shap_descriptors\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "mean_abs = np.abs(shap_values).mean(axis=0)       # (n_features,)\n",
    "order = np.argsort(-mean_abs)                     # 从大到小\n",
    "topK = 20\n",
    "top_idx = order[:topK]\n",
    "top_feats = [X_explain_df.columns[i] for i in top_idx]\n",
    "top_imp   = mean_abs[top_idx]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5.5))\n",
    "y_pos = np.arange(len(top_feats))[::-1]  \n",
    "ax.barh(y_pos, top_imp, color=plt.cm.viridis(np.linspace(0.1, 0.9, len(top_feats))))\n",
    "ax.set_yticks(y_pos); ax.set_yticklabels(top_feats, fontsize=11)\n",
    "ax.set_xlabel(\"Importance Score\", fontsize=12)\n",
    "ax.set_ylabel(\"Feature\", fontsize=12)\n",
    "ax.grid(axis='x', linestyle=':', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(outdir, \"fig_a_global_bar_descriptors_top20.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "bin_means = []\n",
    "for f in top_feats:\n",
    "    v = X_explain_df[f].values\n",
    "    q1, q2 = np.quantile(v, [1/3, 2/3])\n",
    "    low_m   = v <= q1\n",
    "    med_m   = (v > q1) & (v <= q2)\n",
    "    high_m  = v > q2\n",
    "    col_idx = X_explain_df.columns.get_loc(f)\n",
    "    s = np.abs(shap_values[:, col_idx])\n",
    "    bin_means.append([\n",
    "        s[low_m].mean()  if low_m.sum()  > 0 else 0.0,\n",
    "        s[med_m].mean()  if med_m.sum()  > 0 else 0.0,\n",
    "        s[high_m].mean() if high_m.sum() > 0 else 0.0\n",
    "    ])\n",
    "bin_means = np.array(bin_means) \n",
    "\n",
    "low  = bin_means[:, 0]\n",
    "med  = bin_means[:, 1]\n",
    "high = bin_means[:, 2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5.7))\n",
    "y_pos = np.arange(len(top_feats))[::-1]\n",
    "\n",
    "ax.barh(y_pos, low,  color=c_low,  label=\"low\")\n",
    "ax.barh(y_pos, med,  left=low,           color=c_med,  label=\"medium\")\n",
    "ax.barh(y_pos, high, left=low+med,       color=c_high, label=\"high\")\n",
    "\n",
    "ax.set_yticks(y_pos); ax.set_yticklabels(top_feats, fontsize=11)\n",
    "ax.set_xlabel(\"mean(|SHAP value|) (average impact on model output magnitude)\", fontsize=12)\n",
    "ax.grid(axis='x', linestyle=':', alpha=0.3)\n",
    "\n",
    "leg = ax.legend(frameon=False, loc=\"lower right\", fontsize=11, title=\"Feature value\")\n",
    "plt.setp(leg.get_title(), fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(outdir, \"fig_b_binned_meanabs_shap_top20.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
