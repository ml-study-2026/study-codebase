{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7fae4-3d99-4a9c-888d-72b01c783385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "# =================== Config ===================\n",
    "PATH_XLSX = r\"C:\\**\\**\\**\\input.xlsx\"\n",
    "SHEET_NAME = 0\n",
    "\n",
    "ID_COL_CANDIDATES = [\"Name\", \"ID\", \"Molecule\", \"Mol\", \"SMILES\"]\n",
    "\n",
    "MISSING_COL_DROP_THRES = 0.30\n",
    "LOW_VAR_THRES = 1e-8\n",
    "CORR_THRES = 0.90\n",
    "\n",
    "FINGERPRINT_NAME_PATTERNS = [\n",
    "    r\"fingerprint\", r\"\\bfp\\b\", r\"maccs\", r\"pubchem\", r\"ecfp\", r\"fcfp\",\n",
    "    r\"atompair\", r\"torsion\", r\"avalon\", r\"estatefp\", r\"krfp\", r\"subfpc\"\n",
    "]\n",
    "BINARY_AS_FP_MIN_UNIQUE = 2\n",
    "BINARY_RATE_TOL = 0.99\n",
    "\n",
    "WRITE_SUMMARY_SHEET = True\n",
    "\n",
    "def ts() -> str:\n",
    "    return time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def load_df(path, sheet):\n",
    "    return pd.read_excel(path, sheet_name=sheet)\n",
    "\n",
    "\n",
    "def split_id(df: pd.DataFrame, keys: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    id_cols = []\n",
    "    for c in df.columns:\n",
    "        c_str = str(c)\n",
    "        for k in keys:\n",
    "            if re.search(rf\"{re.escape(k)}\", c_str, flags=re.I):\n",
    "                id_cols.append(c)\n",
    "                break\n",
    "    id_cols = list(dict.fromkeys(id_cols))\n",
    "    return (\n",
    "        df[id_cols].copy() if id_cols else pd.DataFrame(index=df.index),\n",
    "        df.drop(columns=id_cols, errors=\"ignore\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def detect_fp_cols(df_num: pd.DataFrame) -> List[str]:\n",
    "    fp = set()\n",
    "    name_re = re.compile(\"|\".join(FINGERPRINT_NAME_PATTERNS), flags=re.I) if FINGERPRINT_NAME_PATTERNS else None\n",
    "\n",
    "    # 1) Column-name heuristics\n",
    "    if name_re:\n",
    "        for c in df_num.columns:\n",
    "            if name_re.search(str(c)):\n",
    "                fp.add(c)\n",
    "\n",
    "    # 2) Binary / near-binary heuristics\n",
    "    for c in df_num.columns:\n",
    "        s = df_num[c]\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            vals = s.dropna().unique()\n",
    "            if len(vals) <= BINARY_AS_FP_MIN_UNIQUE and set(np.round(vals, 6)).issubset({0, 1}):\n",
    "                fp.add(c)\n",
    "                continue\n",
    "            if s.notna().any():\n",
    "                in_01 = s.dropna().isin([0, 1]).mean()\n",
    "                if in_01 >= BINARY_RATE_TOL:\n",
    "                    fp.add(c)\n",
    "\n",
    "    return sorted(fp)\n",
    "\n",
    "\n",
    "def drop_high_missing(df_num: pd.DataFrame, thres: float):\n",
    "    miss = df_num.isna().mean()\n",
    "    drop_cols = miss[miss > thres].index.tolist()\n",
    "    return df_num.drop(columns=drop_cols, errors=\"ignore\"), drop_cols\n",
    "\n",
    "\n",
    "def drop_low_var(df_num: pd.DataFrame, thres: float):\n",
    "    var = df_num.var(skipna=True)\n",
    "    drop_cols = var[var <= thres].index.tolist()\n",
    "    return df_num.drop(columns=drop_cols, errors=\"ignore\"), drop_cols\n",
    "\n",
    "\n",
    "def corr_prune(df_num: pd.DataFrame, thres: float):\n",
    "    df_imp = df_num.copy()\n",
    "    for c in df_imp.columns:\n",
    "        med = df_imp[c].median(skipna=True)\n",
    "        df_imp[c] = df_imp[c].fillna(med)\n",
    "\n",
    "    corr = df_imp.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "\n",
    "    pairs = (\n",
    "        upper.stack()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"level_0\": \"c1\", \"level_1\": \"c2\", 0: \"r\"})\n",
    "        .sort_values(\"r\", ascending=False)\n",
    "    )\n",
    "\n",
    "    kept = set(df_num.columns)\n",
    "    drops_pairs = []\n",
    "    na_ratio = df_num.isna().mean().to_dict()\n",
    "    variance = df_imp.var().to_dict()\n",
    "\n",
    "    def worse_col(a, b):\n",
    "        # Higher NA ratio is worse; higher variance is better (very rough heuristic).\n",
    "        score = lambda x: -(na_ratio.get(x, 0.0)) * 2.0 + variance.get(x, 0.0)\n",
    "        return a if score(a) < score(b) else b\n",
    "\n",
    "    for _, row in pairs.iterrows():\n",
    "        if row[\"r\"] <= thres:\n",
    "            break\n",
    "        a, b = row[\"c1\"], row[\"c2\"]\n",
    "        if a in kept and b in kept:\n",
    "            w = worse_col(a, b)\n",
    "            k = b if w == a else a  # kept one\n",
    "            kept.remove(w)\n",
    "            drops_pairs.append((k, w))\n",
    "\n",
    "    kept_cols = [c for c in df_num.columns if c in kept]\n",
    "    return df_num[kept_cols].copy(), drops_pairs\n",
    "\n",
    "\n",
    "def main():\n",
    "    raw = load_df(PATH_XLSX, SHEET_NAME)\n",
    "    raw.columns = [str(c) for c in raw.columns]\n",
    "\n",
    "    # Convert non-ID columns to numeric\n",
    "    id_keys_lower = [k.lower() for k in ID_COL_CANDIDATES]\n",
    "    for col in raw.columns:\n",
    "        col_str = str(col)\n",
    "        if not any(k in col_str.lower() for k in id_keys_lower):\n",
    "            raw[col] = pd.to_numeric(raw[col], errors=\"coerce\")\n",
    "\n",
    "    out_dir = Path(os.path.dirname(PATH_XLSX) or \".\")\n",
    "    out_xlsx = out_dir / f\"descriptors_dropna_{ts()}.xlsx\"\n",
    "    out_csv = out_dir / f\"descriptors_dropna_{ts()}.csv\"\n",
    "\n",
    "    logs = {}\n",
    "\n",
    "    id_df, feat_df = split_id(raw, ID_COL_CANDIDATES)\n",
    "    logs[\"ID_Columns\"] = list(id_df.columns)\n",
    "\n",
    "    num_cols = [c for c in feat_df.columns if pd.api.types.is_numeric_dtype(feat_df[c])]\n",
    "    non_num = [c for c in feat_df.columns if c not in num_cols]\n",
    "\n",
    "    X = feat_df[num_cols].copy()\n",
    "    logs[\"Dropped_NonNumeric\"] = non_num\n",
    "\n",
    "    fp_cols = detect_fp_cols(X)\n",
    "    X = X.drop(columns=fp_cols, errors=\"ignore\")\n",
    "    logs[\"Dropped_FingerprintLike\"] = fp_cols\n",
    "\n",
    "    X, dropped_miss = drop_high_missing(X, MISSING_COL_DROP_THRES)\n",
    "    logs[\"Dropped_HighMissing\"] = dropped_miss\n",
    "\n",
    "    X, dropped_lowvar = drop_low_var(X, LOW_VAR_THRES)\n",
    "    logs[\"Dropped_LowVariance\"] = dropped_lowvar\n",
    "\n",
    "    if X.shape[1] >= 2:\n",
    "        X, corr_drops = corr_prune(X, CORR_THRES)\n",
    "    else:\n",
    "        corr_drops = []\n",
    "    logs[\"Dropped_CorrPairs(kept->dropped)\"] = [f\"{k} -> {d}\" for (k, d) in corr_drops]\n",
    "\n",
    "    # Drop ALL rows with any NaN (no imputation)\n",
    "    kept_idx = X.dropna(axis=0).index\n",
    "    X_clean = X.loc[kept_idx].reset_index(drop=True)\n",
    "    id_sync = id_df.loc[kept_idx].reset_index(drop=True) if not id_df.empty else id_df\n",
    "    removed_rows = X.shape[0] - X_clean.shape[0]\n",
    "\n",
    "    # Export\n",
    "    with pd.ExcelWriter(out_xlsx, engine=\"openpyxl\") as w:\n",
    "        pd.concat([id_sync, X_clean], axis=1).to_excel(w, sheet_name=\"descriptors_no_missing\", index=False)\n",
    "        pd.DataFrame({\"Kept_Columns\": X_clean.columns.tolist()}).to_excel(w, sheet_name=\"kept_columns\", index=False)\n",
    "\n",
    "        # Logs\n",
    "        for k, v in logs.items():\n",
    "            pd.DataFrame({k: v}).to_excel(w, sheet_name=k[:31], index=False)\n",
    "\n",
    "        # Optional summary sheet (safe to delete/disable)\n",
    "        if WRITE_SUMMARY_SHEET:\n",
    "            summary = {\n",
    "                \"n_rows_raw\": [raw.shape[0]],\n",
    "                \"n_rows_before_dropna\": [X.shape[0]],\n",
    "                \"n_rows_after_dropna\": [X_clean.shape[0]],\n",
    "                \"n_rows_removed\": [removed_rows],\n",
    "                \"n_cols_raw\": [raw.shape[1]],\n",
    "                \"n_numeric_candidate_cols\": [len(num_cols)],\n",
    "                \"n_dropped_non_numeric\": [len(non_num)],\n",
    "                \"n_dropped_fingerprint_like\": [len(fp_cols)],\n",
    "                \"n_dropped_high_missing\": [len(dropped_miss)],\n",
    "                \"n_dropped_low_variance\": [len(dropped_lowvar)],\n",
    "                \"n_dropped_corr_pairs\": [len(corr_drops)],\n",
    "                \"n_final_descriptors\": [X_clean.shape[1]],\n",
    "            }\n",
    "            pd.DataFrame(summary).to_excel(w, sheet_name=\"summary\", index=False)\n",
    "\n",
    "    pd.concat([id_sync, X_clean], axis=1).to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"Filtering completed (all rows containing NaN were removed).\")\n",
    "    print(\"Excel:\", out_xlsx)\n",
    "    print(\"CSV  :\", out_csv)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e009bfbe-6076-487d-8bed-b6d7b04b2737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "IN_DIR = r\"C:\\**\\**\\**\\PaDEL\"\n",
    "INPUT_GLOB = \"structure_descriptors_no_NaN_*.xlsx\"\n",
    "SHEET_NAME = \"structure_descriptors_complete\"\n",
    "\n",
    "ID_COL_CANDIDATES = [\"Name\", \"ID\", \"Molecule\", \"Mol\", \"SMILES\"]\n",
    "\n",
    "TARGET_N   = 35   \n",
    "N_VAR      = 35   \n",
    "N_PCA      = 35  \n",
    "VAR_EXPLAIN = 0.95 \n",
    "\n",
    "def ts():\n",
    "    return time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def latest_file_by_glob(folder, pattern):\n",
    "    candidates = sorted(Path(folder).glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not candidates:\n",
    "raise FileNotFoundError(\n",
    "    f\"No matching files found under {folder}: {pattern}\"\n",
    ")\n",
    "    return str(candidates[0])\n",
    "\n",
    "def split_id_cols(df: pd.DataFrame, keys):\n",
    "    id_cols = []\n",
    "    for c in df.columns:\n",
    "        cs = str(c)\n",
    "        for k in keys:\n",
    "            if k.lower() in cs.lower():\n",
    "                id_cols.append(c); break\n",
    "    id_cols = list(dict.fromkeys(id_cols))\n",
    "    return (df[id_cols].copy() if id_cols else pd.DataFrame(index=df.index),\n",
    "            df.drop(columns=id_cols, errors=\"ignore\"))\n",
    "\n",
    "def main():\n",
    "    input_path = latest_file_by_glob(IN_DIR, INPUT_GLOB)\n",
    "    out_tag = ts()\n",
    "    out_xlsx = Path(IN_DIR) / f\"structure_descriptors_second_dedup_{out_tag}.xlsx\"\n",
    "    out_csv  = Path(IN_DIR) / f\"structure_descriptors_second_dedup_{out_tag}.csv\"\n",
    "\n",
    "    raw = pd.read_excel(input_path, sheet_name=SHEET_NAME)\n",
    "    raw.columns = [str(c) for c in raw.columns]\n",
    "\n",
    "    id_df, X = split_id_cols(raw, ID_COL_CANDIDATES)\n",
    "    X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X = X.dropna(axis=0).reset_index(drop=True)\n",
    "\n",
    "    var_series = X.var(ddof=0)\n",
    "    topvar_cols = var_series.sort_values(ascending=False).head(min(N_VAR, X.shape[1])).index.tolist()\n",
    "\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    Xs = scaler.fit_transform(X.values)\n",
    "    n_comp = min(Xs.shape[0], Xs.shape[1])  # 样本数/特征数较小者\n",
    "    pca = PCA(n_components=n_comp, svd_solver=\"full\", random_state=0)\n",
    "    Xp = pca.fit_transform(Xs)\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    cum = np.cumsum(evr)\n",
    "    k = int(np.searchsorted(cum, VAR_EXPLAIN) + 1)\n",
    "    k = max(1, min(k, n_comp))\n",
    "\n",
    "    loadings = pca.components_.T  # 形状：(n_features, n_components)\n",
    "\n",
    "    pca_scores = (loadings[:, :k] ** 2).sum(axis=1)\n",
    "    pca_scores = pd.Series(pca_scores, index=X.columns, name=\"PCA_Score\")\n",
    "\n",
    "\n",
    "    top_pca_cols = pca_scores.sort_values(ascending=False).head(min(N_PCA, X.shape[1])).index.tolist()\n",
    "\n",
    "\n",
    "    union_cols = list(dict.fromkeys(top_pca_cols + topvar_cols)) \n",
    "\n",
    "    _p = pca_scores.reindex(union_cols).fillna(-1e9)\n",
    "    _v = var_series.reindex(union_cols).fillna(-1e9)\n",
    "    order = sorted(union_cols, key=lambda c: (_p[c], _v[c]), reverse=True)\n",
    "    final_cols = order[:min(TARGET_N, len(order))]\n",
    "\n",
    "    X_sel = X[final_cols].copy()\n",
    "    out_df = pd.concat([id_df.reset_index(drop=True), X_sel.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    info_df = pd.DataFrame({\n",
    "        \"Input_File\": [input_path],\n",
    "        \"Rows\": [X.shape[0]],\n",
    "        \"Cols_Before\": [X.shape[1]],\n",
    "        \"TopVar_Take\": [len(topvar_cols)],\n",
    "        \"PCA_Take\": [len(top_pca_cols)],\n",
    "        \"Final_Cols\": [len(final_cols)],\n",
    "        \"PCA_Used_k\": [k],\n",
    "        \"PCA_CumExplained\": [float(cum[k-1])]\n",
    "    })\n",
    "\n",
    "\n",
    "    var_table = var_series.sort_values(ascending=False).rename(\"Variance\").reset_index().rename(columns={\"index\":\"Feature\"})\n",
    "\n",
    "    pca_table = pca_scores.sort_values(ascending=False).rename(\"PCA_Score\").reset_index().rename(columns={\"index\":\"Feature\"})\n",
    "\n",
    "    final_table = pd.DataFrame({\n",
    "        \"Feature\": final_cols,\n",
    "        \"PCA_Score\": [pca_scores[c] for c in final_cols],\n",
    "        \"Variance\":  [var_series[c] for c in final_cols],\n",
    "        \"Selected_By\": [(\"PCA\" if c in top_pca_cols else \"\") + (\"+VAR\" if c in topvar_cols else \"\") for c in final_cols]\n",
    "    })\n",
    "\n",
    "with pd.ExcelWriter(out_xlsx, engine=\"openpyxl\") as w:\n",
    "    out_df.to_excel(w, sheet_name=\"Deduplicated_Data_Matrix\", index=False)\n",
    "    final_table.to_excel(w, sheet_name=\"Final_Feature_List\", index=False)\n",
    "    var_table.to_excel(w, sheet_name=\"Variance_Ranking\", index=False)\n",
    "    pca_table.to_excel(w, sheet_name=\"PCA_Scores\", index=False)\n",
    "\n",
    "    pd.DataFrame({\n",
    "        \"Component\": np.arange(1, len(evr) + 1),\n",
    "        \"ExplainedVarianceRatio\": evr,\n",
    "        \"Cumulative\": cum\n",
    "    }).to_excel(w, sheet_name=\"PCA_Explained_Variance\", index=False)\n",
    "    info_df.to_excel(w, sheet_name=\"Run_Info\", index=False)\n",
    "\n",
    "out_df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Second-stage deduplication completed\")\n",
    "print(\"Input:\", input_path)\n",
    "print(\"Output:\", out_xlsx)\n",
    "print(\"CSV:\", out_csv)\n",
    "print(\n",
    "    f\"Final retained {len(final_cols)} features (target {TARGET_N}) | \"\n",
    "    f\"Cumulative explained variance of first {k} PCs before PCA = {cum[k-1]:.4f}\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
