{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e73f1-5875-4946-8b61-d226f1cf1802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"TORCH_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"TORCH_CPP_LOG_LEVEL\"] = \"ERROR\"\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "torch.set_num_interop_threads(1)\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, joblib\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "SEED = 2025\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "BASE = r\"C:\\Users\\81005\\Desktop\\CYH\\3-PFAS\"\n",
    "PATH_TRAIN = os.path.join(BASE, r\"deep_learning_validation\\merged_all.xlsx\")\n",
    "PATH_NEW   = os.path.join(BASE, r\"large_scale_prediction\\7565_filled.xlsx\")\n",
    "OUT_DIR    = os.path.join(BASE, r\"large_scale_prediction\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "df_tr = pd.read_excel(PATH_TRAIN)\n",
    "assert \"Level_OA\" in df_tr.columns\n",
    "df_tr[\"Level_OA\"] = df_tr[\"Level_OA\"].astype(int)\n",
    "df_new = pd.read_excel(PATH_NEW)\n",
    "\n",
    "EXCLUDE = {\"Name\",\"Level_OA\",\"fold_id\",\"G\",\"P_PI3K\",\"P_PPAR\",\"P_ROS\",\"P_LPS\",\"P_OA\"}\n",
    "num_cols_tr  = [c for c in df_tr.columns  if (c not in EXCLUDE and np.issubdtype(df_tr[c].dtype, np.number))]\n",
    "num_cols_new = [c for c in df_new.columns if  np.issubdtype(df_new[c].dtype, np.number)]\n",
    "COLS_FINAL   = [c for c in num_cols_tr if c in num_cols_new]\n",
    "assert len(COLS_FINAL) > 0, \"No common structural feature columns between training data and new data\"\n",
    "\n",
    "X_all_raw = df_tr[COLS_FINAL].replace([np.inf,-np.inf], np.nan).values\n",
    "y_all = df_tr[\"Level_OA\"].values\n",
    "X_new_raw = df_new[COLS_FINAL].replace([np.inf,-np.inf], np.nan).values\n",
    "\n",
    "y_min = int(y_all.min())\n",
    "y_all0 = (y_all - y_min).astype(int)\n",
    "n_classes = int(len(np.unique(y_all0)))\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=SEED)\n",
    "tr_idx, val_idx = next(sss.split(X_all_raw, y_all0))\n",
    "X_tr_raw, X_val_raw = X_all_raw[tr_idx], X_all_raw[val_idx]\n",
    "y_tr, y_val = y_all0[tr_idx], y_all0[val_idx]\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\").fit(X_tr_raw)\n",
    "X_tr_imp  = imputer.transform(X_tr_raw)\n",
    "X_val_imp = imputer.transform(X_val_raw)\n",
    "\n",
    "scaler = StandardScaler().fit(X_tr_imp)\n",
    "X_tr  = scaler.transform(X_tr_imp)\n",
    "X_val = scaler.transform(X_val_imp)\n",
    "\n",
    "X_all_std = scaler.transform(imputer.transform(X_all_raw))\n",
    "X_new_std = scaler.transform(imputer.transform(X_new_raw))\n",
    "\n",
    "joblib.dump(imputer, os.path.join(OUT_DIR, \"imputer.joblib\"))\n",
    "joblib.dump(scaler,  os.path.join(OUT_DIR, \"scaler.joblib\"))\n",
    "joblib.dump(COLS_FINAL, os.path.join(OUT_DIR, \"cols_final.joblib\"))\n",
    "\n",
    "class TinyTabTransformer(nn.Module):\n",
    "    def __init__(self, in_dim, n_classes=3, d_model=64, n_heads=4, n_layers=2, dropout=0.35):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, d_model)\n",
    "        enc = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads,\n",
    "            dim_feedforward=d_model*2, dropout=dropout,\n",
    "            batch_first=True, activation=\"gelu\"\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model//2),\n",
    "            nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(d_model//2, n_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).unsqueeze(1)\n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x.squeeze(1))\n",
    "        return self.head(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TinyTabTransformer(in_dim=X_tr.shape[1], n_classes=n_classes,\n",
    "                           d_model=64, n_heads=4, n_layers=2, dropout=0.35).to(device)\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(X_tr, dtype=torch.float32),\n",
    "                         torch.tensor(y_tr, dtype=torch.long))\n",
    "val_ds   = TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                         torch.tensor(y_val, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,\n",
    "                          num_workers=0, pin_memory=False, persistent_workers=False)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False,\n",
    "                          num_workers=0, pin_memory=False, persistent_workers=False)\n",
    "\n",
    "cnt_tr = np.bincount(y_tr, minlength=n_classes).astype(np.float32)\n",
    "cnt_tr = np.maximum(cnt_tr, 1.0)\n",
    "w_tr = (len(y_tr) / cnt_tr)\n",
    "w_tr = w_tr / w_tr.mean()\n",
    "w_tr = torch.tensor(w_tr, dtype=torch.float32, device=device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=w_tr)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "best_f1, best_state, no_improve, patience = -1.0, None, 0, 50\n",
    "\n",
    "for ep in range(500):\n",
    "    model.train()\n",
    "    for bx, by in train_loader:\n",
    "        bx, by = bx.to(device), by.to(device)\n",
    "        logits = model(bx)\n",
    "        loss = criterion(logits, by)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pv, yv = [], []\n",
    "        for bx, by in val_loader:\n",
    "            bx = bx.to(device)\n",
    "            logit = model(bx)\n",
    "            pv.append(logit.argmax(1).cpu().numpy())\n",
    "            yv.append(by.numpy())\n",
    "\n",
    "    f1 = f1_score(np.concatenate(yv), np.concatenate(pv), average=\"macro\")\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            break\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "cnt_all = np.bincount(y_all0, minlength=n_classes).astype(np.float32)\n",
    "cnt_all = np.maximum(cnt_all, 1.0)\n",
    "w_all = (len(y_all0) / cnt_all)\n",
    "w_all = w_all / w_all.mean()\n",
    "w_all = torch.tensor(w_all, dtype=torch.float32, device=device)\n",
    "\n",
    "criterion_final = nn.CrossEntropyLoss(weight=w_all)\n",
    "optim_final = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "full_ds = TensorDataset(torch.tensor(X_all_std, dtype=torch.float32),\n",
    "                        torch.tensor(y_all0, dtype=torch.long))\n",
    "full_loader = DataLoader(full_ds, batch_size=32, shuffle=True,\n",
    "                         num_workers=0, pin_memory=False, persistent_workers=False)\n",
    "\n",
    "for ep in range(10):\n",
    "    model.train()\n",
    "    for bx, by in full_loader:\n",
    "        bx, by = bx.to(device), by.to(device)\n",
    "        loss = criterion_final(model(bx), by)\n",
    "        optim_final.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim_final.step()\n",
    "\n",
    "A = 5\n",
    "pca = PCA(n_components=A, random_state=SEED).fit(X_all_std)\n",
    "T_train = pca.transform(X_all_std)   \n",
    "cov_T = np.cov(T_train, rowvar=False)\n",
    "inv_cov_T = np.linalg.pinv(cov_T)\n",
    "\n",
    "h_train = np.einsum('ij,jk,ik->i', T_train, inv_cov_T, T_train)\n",
    "h_star = np.quantile(h_train, 0.95)\n",
    "\n",
    "T_new = pca.transform(X_new_std)\n",
    "h_new = np.einsum('ij,jk,ik->i', T_new, inv_cov_T, T_new)\n",
    "ad_in = (h_new <= h_star)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_new = model(torch.tensor(X_new_std, dtype=torch.float32, device=device)).cpu()\n",
    "prob = torch.softmax(logits_new, dim=1).numpy()           \n",
    "pred0 = prob.argmax(axis=1)                              \n",
    "pred = pred0 + y_min                                     \n",
    "\n",
    "prob_cols = [f\"Prob_Level_{i}\" for i in range(1, n_classes+1)]\n",
    "out = pd.concat([\n",
    "    df_new.reset_index(drop=True),\n",
    "    pd.DataFrame({\"Pred_Level\": pred, \"AD_in\": ad_in}),\n",
    "    pd.DataFrame(prob, columns=prob_cols)\n",
    "], axis=1)\n",
    "out[\"max_prob\"] = out[prob_cols].max(axis=1)\n",
    "\n",
    "out.to_excel(os.path.join(OUT_DIR, \"predictions_Xonly.xlsx\"), index=False)\n",
    "print(\"[OK] 保存：predictions_Xonly.xlsx\")\n",
    "\n",
    "risk_low  = out[out[\"Pred_Level\"]==1].sort_values(\"Prob_Level_1\", ascending=False)\n",
    "risk_mid  = out[out[\"Pred_Level\"]==2].sort_values(\"Prob_Level_2\", ascending=False)\n",
    "risk_high = out[out[\"Pred_Level\"]==3].sort_values(\"Prob_Level_3\", ascending=False)\n",
    "\n",
    "risk_low.to_excel (os.path.join(OUT_DIR, \"risk_low_L1_all.xlsx\"),  index=False)\n",
    "risk_mid.to_excel (os.path.join(OUT_DIR, \"risk_mid_L2_all.xlsx\"),  index=False)\n",
    "risk_high.to_excel(os.path.join(OUT_DIR, \"risk_high_L3_all.xlsx\"), index=False)\n",
    "print(\"[OK]save：L1/L2/L3 \")\n",
    "\n",
    "core = out[(out[\"AD_in\"]) & (out[\"max_prob\"]>=0.80)].copy()\n",
    "core[core[\"Pred_Level\"]==1].sort_values(\"Prob_Level_1\", ascending=False)\\\n",
    "    .to_excel(os.path.join(OUT_DIR,\"risk_low_L1_core.xlsx\"), index=False)\n",
    "core[core[\"Pred_Level\"]==2].sort_values(\"Prob_Level_2\", ascending=False)\\\n",
    "    .to_excel(os.path.join(OUT_DIR,\"risk_mid_L2_core.xlsx\"), index=False)\n",
    "core[core[\"Pred_Level\"]==3].sort_values(\"Prob_Level_3\", ascending=False)\\\n",
    "    .to_excel(os.path.join(OUT_DIR,\"risk_high_L3_core.xlsx\"), index=False)\n",
    "print(\"[OK] save：L1/L2/L3 \")\n",
    "\n",
    "SUMMARY_TXT  = os.path.join(OUT_DIR, \"AD_summary.txt\")\n",
    "SUMMARY_XLSX = os.path.join(OUT_DIR, \"AD_breakdown.xlsx\")\n",
    "\n",
    "out[\"ICP_flag\"] = np.select(\n",
    "    [out[\"max_prob\"]>=0.80, out[\"max_prob\"]>=0.60],\n",
    "    [\"HighConf\",\"Borderline\"], default=\"Empty\"\n",
    ")\n",
    "n_total          = len(out)\n",
    "cov_leverage     = float(out[\"AD_in\"].mean())\n",
    "cov_highconf_all = float((out[\"max_prob\"]>=0.80).mean())\n",
    "cov_highconf_in  = float(((out[\"AD_in\"]) & (out[\"max_prob\"]>=0.80)).mean())\n",
    "\n",
    "print(\"Number of samples:\", n_total)\n",
    "print(\"Leverage h*:\", float(h_star))\n",
    "print(\"Leverage coverage:\", round(cov_leverage, 3))\n",
    "print(\"HighConf (≥0.80) overall proportion:\", round(cov_highconf_all, 3))\n",
    "print(\"HighConf (≥0.80) proportion within AD_in:\", round(cov_highconf_in, 3))\n",
    "print(\"ICP_flag distribution:\\n\", out[\"ICP_flag\"].value_counts(normalize=True).round(3))\n",
    "\n",
    "with open(SUMMARY_TXT,\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(f\"n_total={n_total}\\n\")\n",
    "    f.write(f\"h_star={float(h_star):.6f}\\n\")\n",
    "    f.write(f\"cov_leverage={cov_leverage:.3f}\\n\")\n",
    "    f.write(f\"highconf_all={cov_highconf_all:.3f}\\n\")\n",
    "    f.write(f\"highconf_in_AD={cov_highconf_in:.3f}\\n\")\n",
    "    f.write(\"ICP_flag:\\n\")\n",
    "    for k,v in out[\"ICP_flag\"].value_counts(normalize=True).items():\n",
    "        f.write(f\"  {k}: {v:.3f}\\n\")\n",
    "\n",
    "overall_df = pd.DataFrame({\n",
    "    \"metric\":[\"n_total\",\"h_star\",\"cov_leverage\",\"highconf_all\",\"highconf_in_AD\"],\n",
    "    \"value\":[n_total, float(h_star), cov_leverage, cov_highconf_all, cov_highconf_in]\n",
    "})\n",
    "by_icp = out[\"ICP_flag\"].value_counts().to_frame(\"count\")\n",
    "by_icp[\"proportion\"] = by_icp[\"count\"]/n_total\n",
    "\n",
    "with pd.ExcelWriter(SUMMARY_XLSX) as w:\n",
    "    overall_df.to_excel(w, sheet_name=\"overall\", index=False)\n",
    "    by_icp.to_excel(w, sheet_name=\"by_ICP_flag\")\n",
    "    out[[\"AD_in\",\"ICP_flag\",\"max_prob\"]+prob_cols].head(20)\\\n",
    "        .to_excel(w, sheet_name=\"preview_top20\", index=False)\n",
    "\n",
    "print(\"[OK] save：AD_summary.txt / AD_breakdown.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
